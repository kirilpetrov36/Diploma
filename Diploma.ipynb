{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library for understanding music\n",
    "from music21 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Musical Files:\n",
    "\n",
    "Letâ€™s define a function straight away for reading the MIDI files. It returns the array of notes and chords present in the musical file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to read MIDI files\n",
    "def read_midi(file):\n",
    "    \n",
    "    print(\"Loading Music File:\",file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  \n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the MIDI files into our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schubert_D850_1.mid\n",
      "Loading Music File: schubert/schubert_D850_2.mid\n",
      "Loading Music File: schubert/schubert_D850_3.mid\n",
      "Loading Music File: schubert/schubert_D850_4.mid\n",
      "Loading Music File: schubert/schubert_D935_1.mid\n",
      "Loading Music File: schubert/schubert_D935_2.mid\n",
      "Loading Music File: schubert/schubert_D935_3.mid\n",
      "Loading Music File: schubert/schubert_D935_4.mid\n",
      "Loading Music File: schubert/schub_d760_1.mid\n",
      "Loading Music File: schubert/schub_d760_2.mid\n",
      "Loading Music File: schubert/schub_d760_3.mid\n",
      "Loading Music File: schubert/schub_d760_4.mid\n",
      "Loading Music File: schubert/schub_d960_1.mid\n",
      "Loading Music File: schubert/schub_d960_2.mid\n",
      "Loading Music File: schubert/schub_d960_3.mid\n",
      "Loading Music File: schubert/schub_d960_4.mid\n",
      "Loading Music File: schubert/schuim-1.mid\n",
      "Loading Music File: schubert/schuim-2.mid\n",
      "Loading Music File: schubert/schuim-3.mid\n",
      "Loading Music File: schubert/schuim-4.mid\n",
      "Loading Music File: schubert/schumm-1.mid\n",
      "Loading Music File: schubert/schumm-2.mid\n",
      "Loading Music File: schubert/schumm-3.mid\n",
      "Loading Music File: schubert/schumm-4.mid\n",
      "Loading Music File: schubert/schumm-5.mid\n",
      "Loading Music File: schubert/schumm-6.mid\n",
      "Loading Music File: schubert/schu_143_1.mid\n",
      "Loading Music File: schubert/schu_143_2.mid\n",
      "Loading Music File: schubert/schu_143_3.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-13269b8794bc>:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  notes_array = np.array([read_midi(path+i) for i in files])\n"
     ]
    }
   ],
   "source": [
    "#for listing down the file names\n",
    "import os\n",
    "\n",
    "#Array Processing\n",
    "import numpy as np\n",
    "\n",
    "#specify the path\n",
    "path='schubert/'\n",
    "\n",
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi(path+i) for i in files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the data:\n",
    "\n",
    "Under this section, we will explore the dataset and understand it in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n"
     ]
    }
   ],
   "source": [
    "#converting 2D array into 1D array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "\n",
    "#No. of unique notes\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see the distribution of the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([221.,  40.,  31.,  13.,   8.,   3.,   7.,   9.,   7.,   6.]),\n",
       " array([1.0000e+00, 1.6170e+02, 3.2240e+02, 4.8310e+02, 6.4380e+02,\n",
       "        8.0450e+02, 9.6520e+02, 1.1259e+03, 1.2866e+03, 1.4473e+03,\n",
       "        1.6080e+03]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 302,
       "width": 319
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing library\n",
    "from collections import Counter\n",
    "\n",
    "#computing frequency of each note\n",
    "freq = dict(Counter(notes_))\n",
    "\n",
    "#library for visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#consider only the frequencies\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#set the figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#plot\n",
    "plt.hist(no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we can infer that most of the notes have a very low frequency. So, let us keep the top frequent notes and ignore the low-frequency ones. Here, I am defining the threshold as 50. Nevertheless, the parameter can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, no. of frequently occurring notes is around 170.  Now, let us prepare new musical files which contain only the top frequent notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp=[]\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)            \n",
    "    new_music.append(temp)\n",
    "    \n",
    "new_music = np.array(new_music, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Data:\n",
    "\n",
    "Preparing the input and output sequences as mentioned in the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assign a unique integer to every note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will prepare the integer sequences for input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing input sequences\n",
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, prepare the integer sequences for output data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq=np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us preserve 80% of the data for training and the rest 20% for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building\n",
    "\n",
    "I have defined 2 architectures here â€“ WaveNet and LSTM. Please experiment with both the architectures to understand the importance of WaveNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           18200     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 182)               46774     \n",
      "=================================================================\n",
      "Total params: 273,294\n",
      "Trainable params: 273,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "    \n",
    "#embedding layer\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "    \n",
    "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "          \n",
    "#model.add(Conv1D(256,5,activation='relu'))    \n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(unique_y), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the callback to save the best model during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s train the model with a batch size of 128 for 50 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "492/492 [==============================] - 69s 140ms/step - loss: 2.5916 - val_loss: 3.1134\n",
      "\n",
      "Epoch 00001: val_loss improved from 3.11942 to 3.11343, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "492/492 [==============================] - 67s 135ms/step - loss: 2.5995 - val_loss: 3.1229\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 3.11343\n",
      "Epoch 3/50\n",
      "492/492 [==============================] - 72s 146ms/step - loss: 2.5922 - val_loss: 3.1169\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.11343\n",
      "Epoch 4/50\n",
      "492/492 [==============================] - 63s 129ms/step - loss: 2.5832 - val_loss: 3.1237\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.11343\n",
      "Epoch 5/50\n",
      "492/492 [==============================] - 58s 119ms/step - loss: 2.5823 - val_loss: 3.1114\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.11343 to 3.11139, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      "492/492 [==============================] - 58s 117ms/step - loss: 2.5754 - val_loss: 3.1311\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.11139\n",
      "Epoch 7/50\n",
      "492/492 [==============================] - 57s 116ms/step - loss: 2.5699 - val_loss: 3.1215\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.11139\n",
      "Epoch 8/50\n",
      "492/492 [==============================] - 59s 120ms/step - loss: 2.5688 - val_loss: 3.1137\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.11139\n",
      "Epoch 9/50\n",
      "492/492 [==============================] - 59s 119ms/step - loss: 2.5631 - val_loss: 3.1090\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.11139 to 3.10900, saving model to best_model.h5\n",
      "Epoch 10/50\n",
      "492/492 [==============================] - 59s 121ms/step - loss: 2.5549 - val_loss: 3.1146\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.10900\n",
      "Epoch 11/50\n",
      "492/492 [==============================] - 65s 131ms/step - loss: 2.5602 - val_loss: 3.1110\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 3.10900\n",
      "Epoch 12/50\n",
      "492/492 [==============================] - 68s 139ms/step - loss: 2.5521 - val_loss: 3.1208\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.10900\n",
      "Epoch 13/50\n",
      "492/492 [==============================] - 65s 132ms/step - loss: 2.5497 - val_loss: 3.1066\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.10900 to 3.10661, saving model to best_model.h5\n",
      "Epoch 14/50\n",
      "492/492 [==============================] - 61s 123ms/step - loss: 2.5496 - val_loss: 3.1181\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 3.10661\n",
      "Epoch 15/50\n",
      "492/492 [==============================] - 61s 125ms/step - loss: 2.5398 - val_loss: 3.1047\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.10661 to 3.10465, saving model to best_model.h5\n",
      "Epoch 16/50\n",
      "492/492 [==============================] - 62s 125ms/step - loss: 2.5357 - val_loss: 3.1063\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.10465\n",
      "Epoch 17/50\n",
      "492/492 [==============================] - 61s 124ms/step - loss: 2.5345 - val_loss: 3.1202\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 3.10465\n",
      "Epoch 18/50\n",
      "492/492 [==============================] - 61s 124ms/step - loss: 2.5256 - val_loss: 3.0988\n",
      "\n",
      "Epoch 00018: val_loss improved from 3.10465 to 3.09878, saving model to best_model.h5\n",
      "Epoch 19/50\n",
      "492/492 [==============================] - 61s 124ms/step - loss: 2.5320 - val_loss: 3.1045\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.09878\n",
      "Epoch 20/50\n",
      "492/492 [==============================] - 59s 120ms/step - loss: 2.5257 - val_loss: 3.1030\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.09878\n",
      "Epoch 21/50\n",
      "492/492 [==============================] - 58s 118ms/step - loss: 2.5190 - val_loss: 3.0936\n",
      "\n",
      "Epoch 00021: val_loss improved from 3.09878 to 3.09364, saving model to best_model.h5\n",
      "Epoch 22/50\n",
      "492/492 [==============================] - 59s 121ms/step - loss: 2.5163 - val_loss: 3.1045\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 3.09364\n",
      "Epoch 23/50\n",
      "492/492 [==============================] - 58s 117ms/step - loss: 2.5108 - val_loss: 3.1057\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 3.09364\n",
      "Epoch 24/50\n",
      "492/492 [==============================] - 58s 118ms/step - loss: 2.5160 - val_loss: 3.1041\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 3.09364\n",
      "Epoch 25/50\n",
      "492/492 [==============================] - 58s 117ms/step - loss: 2.5059 - val_loss: 3.1075\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 3.09364\n",
      "Epoch 26/50\n",
      "492/492 [==============================] - 58s 118ms/step - loss: 2.5067 - val_loss: 3.1056\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 3.09364\n",
      "Epoch 27/50\n",
      "492/492 [==============================] - 58s 117ms/step - loss: 2.5016 - val_loss: 3.0932\n",
      "\n",
      "Epoch 00027: val_loss improved from 3.09364 to 3.09320, saving model to best_model.h5\n",
      "Epoch 28/50\n",
      "492/492 [==============================] - 58s 118ms/step - loss: 2.5048 - val_loss: 3.0996\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 3.09320\n",
      "Epoch 29/50\n",
      "492/492 [==============================] - 58s 118ms/step - loss: 2.4888 - val_loss: 3.0954\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 3.09320\n",
      "Epoch 30/50\n",
      "492/492 [==============================] - 59s 120ms/step - loss: 2.4985 - val_loss: 3.0893\n",
      "\n",
      "Epoch 00030: val_loss improved from 3.09320 to 3.08928, saving model to best_model.h5\n",
      "Epoch 31/50\n",
      "492/492 [==============================] - 58s 118ms/step - loss: 2.4952 - val_loss: 3.1082\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 3.08928\n",
      "Epoch 32/50\n",
      "492/492 [==============================] - 60s 122ms/step - loss: 2.4918 - val_loss: 3.0940\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 3.08928\n",
      "Epoch 33/50\n",
      "492/492 [==============================] - 59s 120ms/step - loss: 2.4915 - val_loss: 3.0957\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 3.08928\n",
      "Epoch 34/50\n",
      "492/492 [==============================] - 59s 119ms/step - loss: 2.4839 - val_loss: 3.0986\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 3.08928\n",
      "Epoch 35/50\n",
      "492/492 [==============================] - 58s 117ms/step - loss: 2.4840 - val_loss: 3.0934\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 3.08928\n",
      "Epoch 36/50\n",
      "492/492 [==============================] - 59s 119ms/step - loss: 2.4806 - val_loss: 3.0933\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 3.08928\n",
      "Epoch 37/50\n",
      "492/492 [==============================] - 58s 117ms/step - loss: 2.4794 - val_loss: 3.0992\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 3.08928\n",
      "Epoch 38/50\n",
      "492/492 [==============================] - 58s 119ms/step - loss: 2.4822 - val_loss: 3.0992\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 3.08928\n",
      "Epoch 39/50\n",
      "492/492 [==============================] - 59s 121ms/step - loss: 2.4788 - val_loss: 3.0945\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 3.08928\n",
      "Epoch 40/50\n",
      "492/492 [==============================] - 60s 123ms/step - loss: 2.4715 - val_loss: 3.1066\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 3.08928\n",
      "Epoch 41/50\n",
      "492/492 [==============================] - 60s 123ms/step - loss: 2.4742 - val_loss: 3.0905\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 3.08928\n",
      "Epoch 42/50\n",
      "492/492 [==============================] - 61s 124ms/step - loss: 2.4649 - val_loss: 3.0950\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 3.08928\n",
      "Epoch 43/50\n",
      "492/492 [==============================] - 61s 125ms/step - loss: 2.4708 - val_loss: 3.0990\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 3.08928\n",
      "Epoch 44/50\n",
      "492/492 [==============================] - 62s 127ms/step - loss: 2.4617 - val_loss: 3.1011\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 3.08928\n",
      "Epoch 45/50\n",
      "492/492 [==============================] - 61s 125ms/step - loss: 2.4643 - val_loss: 3.0882\n",
      "\n",
      "Epoch 00045: val_loss improved from 3.08928 to 3.08815, saving model to best_model.h5\n",
      "Epoch 46/50\n",
      "492/492 [==============================] - 61s 124ms/step - loss: 2.4545 - val_loss: 3.0932\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 3.08815\n",
      "Epoch 47/50\n",
      "492/492 [==============================] - 60s 122ms/step - loss: 2.4626 - val_loss: 3.0875\n",
      "\n",
      "Epoch 00047: val_loss improved from 3.08815 to 3.08753, saving model to best_model.h5\n",
      "Epoch 48/50\n",
      "492/492 [==============================] - 61s 125ms/step - loss: 2.4497 - val_loss: 3.0956\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 3.08753\n",
      "Epoch 49/50\n",
      "492/492 [==============================] - 62s 125ms/step - loss: 2.4598 - val_loss: 3.0897\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 3.08753\n",
      "Epoch 50/50\n",
      "492/492 [==============================] - 61s 123ms/step - loss: 2.4471 - val_loss: 3.0958\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 3.08753\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best model\n",
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its time to compose our own music now. We will follow the steps mentioned under the inference phase for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130, 173, 130, 173, 130, 173, 130, 173, 130, 173]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "ind = np.random.randint(0,len(x_val)-1)\n",
    "\n",
    "random_music = x_val[ind]\n",
    "\n",
    "predictions=[]\n",
    "for i in range(10):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will convert the integers back into the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to convert back the predictions into a MIDI file. Letâ€™s define the function to accomplish the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='music.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the predictions into a musical file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
